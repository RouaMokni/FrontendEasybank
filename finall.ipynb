{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement json (from versions: none)\n",
      "ERROR: No matching distribution found for json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\users\\yassineland\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install json\n",
    "%pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def removeDiacritics(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text.lower()) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Chemin du dossier où sauvegarder le fichier\n",
    "output_folder = \"./translated_data\"\n",
    "\n",
    "# Créer le dossier s'il n'existe pas déjà\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Charger le fichier JSON avec les données originales\n",
    "with open(\"updated_format.json\", encoding=\"utf8\") as file:\n",
    "    discussions = json.load(file)\n",
    "\n",
    "# Charger le fichier Excel et créer un dictionnaire de traduction\n",
    "data = pd.read_excel('./Book1.xlsx')\n",
    "# Replace NaN values with a placeholder or remove rows with NaN in relevant columns\n",
    "data = data.dropna(subset=['Column3', 'Column5', 'Column8'])\n",
    "data['Column3'].apply(removeDiacritics)\n",
    "data['Column5'].apply(removeDiacritics)\n",
    "data['Column8'].apply(removeDiacritics)\n",
    "arabish_coda_dict = dict(zip(data['Column3'], data['Column5'].astype(str)))\n",
    "normm_lemma = dict(zip(data['Column5'].astype(str), data['Column8'].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def replace_arabic_with_coda(tokens, arabish_coda_dict):\n",
    "#     \"\"\"Traduire les tokens en utilisant le dictionnaire.\"\"\"\n",
    "#     new_tokens = []\n",
    "#     for token in tokens:\n",
    "#         if token in arabish_coda_dict:\n",
    "#             new_tokens.append(arabish_coda_dict[token])\n",
    "#         else:\n",
    "#             new_tokens.append(token)\n",
    "#     return new_tokens\n",
    "\n",
    "def replace_arabic_with_coda(tokens, arabish_coda_dict):\n",
    "    \"\"\"Traduire les tokens en utilisant le dictionnaire.\"\"\"\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        token = removeDiacritics(token)\n",
    "        arabish = arabish_coda_dict.get(token, token).strip()\n",
    "        new_tokens.append(arabish)\n",
    "    return new_tokens\n",
    "\n",
    "# def normalize_tokens(tokens, normm_lemma):\n",
    "#     \"\"\"Normaliser les tokens en utilisant le dictionnaire.\"\"\"\n",
    "#     new_tokens = []\n",
    "#     for token in tokens:\n",
    "#         if token in normm_lemma:\n",
    "#             new_tokens.append(normm_lemma[token])\n",
    "#         else:\n",
    "#             new_tokens.append(token)\n",
    "#     return new_tokens\n",
    "def normalize_tokens(tokens, normm_lemma):\n",
    "    \"\"\"Normaliser les tokens en utilisant le dictionnaire.\"\"\"\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        lemma = normm_lemma.get(token, token).strip()\n",
    "        if lemma == \"PUNC\":\n",
    "            continue\n",
    "        if lemma == \"foreign\" or lemma == \"NOUN_NUM\":\n",
    "            new_tokens.append(token)\n",
    "        else:\n",
    "            new_tokens.append(lemma)\n",
    "    return new_tokens\n",
    "\n",
    "# def normalize_tokens(tokens):\n",
    "    \n",
    "#     # Créer un dictionnaire de correspondance clé-valeur à partir des colonnes 5 et 8\n",
    "#     mapping = dict(zip(data['Column5'], data['Column8']))\n",
    "    \n",
    "#     # Normaliser les tokens en utilisant le dictionnaire de correspondance\n",
    "#     normalized_tokens = [mapping.get(token, token) for token in tokens]\n",
    "    \n",
    "#     return normalized_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Traiter chaque discussion pour traduire les questions et les réponses\n",
    "for discussion in discussions:\n",
    "    for utterance in discussion:\n",
    "      \n",
    "      # Translitération de la question\n",
    "        question = removeDiacritics(utterance[\"question\"])\n",
    "        question_tokens = re.findall(r\"(\\w+|[^\\s]+)\", question)\n",
    "        translated_question_tokens = replace_arabic_with_coda(question_tokens, arabish_coda_dict)\n",
    "        utterance[\"question\"] = \" \".join(map(str, translated_question_tokens))  # Ensure all items are strings\n",
    "        \n",
    "        \n",
    "        # Normalisation de la question\n",
    "        normalized_question_tokens = normalize_tokens(translated_question_tokens, normm_lemma)\n",
    "        utterance[\"question_norm\"] = \" \".join(map(str, normalized_question_tokens))  # Ensure all items are strings\n",
    "\n",
    "        # Translitération de la réponse\n",
    "        answer = removeDiacritics(utterance[\"reponse\"])\n",
    "        answer_tokens = re.findall(r\"(\\w+|[^\\s]+)\", answer)\n",
    "        translated_answer_tokens = replace_arabic_with_coda(answer_tokens, arabish_coda_dict)\n",
    "        utterance[\"reponse\"] = \" \".join(map(str, translated_answer_tokens))  # Ensure all items are strings\n",
    "\n",
    "        # Affichage des résultats pour vérification\n",
    "        #print(\"Translated Question:\", utterance[\"question\"])\n",
    "        #print(\"Translated Question Norm:\", utterance[\"question_norm\"])\n",
    "        #print(\"Translated Answer:\", utterance[\"reponse\"])\n",
    "# for discussion in discussions:\n",
    "#     for utterance in discussion:\n",
    "#         # Traduction de la question\n",
    "#         question = utterance[\"question\"].lower()\n",
    "#         question_tokens = re.findall(r\"(\\w+|[^\\s]+)\", question)\n",
    "#         translated_question_tokens = replace_arabic_with_coda(question_tokens, arabish_coda_dict)\n",
    "#         normalized_question_tokens = normalize_tokens(translated_question_tokens, normm_lemma)\n",
    "#         utterance[\"question\"] = \" \".join(normalized_question_tokens)\n",
    "\n",
    "#         # Traduction de la réponse\n",
    "#         answer = utterance[\"reponse\"].lower()\n",
    "#         answer_tokens = re.findall(r\"(\\w+|[^\\s]+)\", answer)\n",
    "#         translated_answer_tokens = replace_arabic_with_coda(answer_tokens, arabish_coda_dict)\n",
    "#         normalized_response_tokens = normalize_tokens(translated_answer_tokens, normm_lemma)\n",
    "#         utterance[\"reponse\"] = \" \".join(normalized_response_tokens)\n",
    "\n",
    "#         # Affichage des résultats pour vérification\n",
    "#         print(\"Translated Question:\", utterance[\"question\"])\n",
    "#         print(\"Translated Answer:\", utterance[\"reponse\"])\n",
    "        \n",
    "\n",
    "    # Utiliser les questions et réponses traduites et normalisées pour le traitement ultérieur\n",
    "    # ...\n",
    "        \n",
    "        # updates.append((translated_question_tokens + translated_answer_tokens, q_pos + a_pos, q_lemmas + a_lemmas))\n",
    "        \n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder les modifications dans un nouveau fichier JSON dans le dossier créé\n",
    "output_path = \"./res.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf8\") as file:\n",
    "    json.dump(discussions, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def read_documents(file_path):\n",
    "    discussions = json.load(open(file_path, 'r', encoding='utf-8'))\n",
    "    \n",
    "    #for i, discussion in enumerate(discussions):\n",
    "    #    for j, itturance in enumerate(discussion):\n",
    "    #        documents[f\"{i}_{j}\"] = itturance\n",
    "    return discussions\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set(set1).intersection(set(set2)))\n",
    "    union = len(set(set1).union(set(set2)))\n",
    "    return intersection / union\n",
    "\n",
    "# Prétraiter la requête pour TunBERT\n",
    "def pre_query_bert(query, arabish_coda_dict):\n",
    "    question = removeDiacritics(query)\n",
    "    question_tokens = re.findall(r\"(\\w+|[^\\s]+)\", question)\n",
    "    return replace_arabic_with_coda(question_tokens, arabish_coda_dict)\n",
    "\n",
    "# Normaliser la requête pour Jaccard\n",
    "def norm_query_jaccard(query, normm_lemma):\n",
    "\t# Normalisation de la question\n",
    "    question = removeDiacritics(query)\n",
    "    question_tokens = re.findall(r\"(\\w+|[^\\s]+)\", question)\n",
    "    translated_question_tokens = replace_arabic_with_coda(question_tokens, arabish_coda_dict)\n",
    "    return normalize_tokens(translated_question_tokens, normm_lemma)\n",
    "\n",
    "# S1 : nheb ne5edh 9ardh -> hab 5dhe 9ardh\n",
    "# S2 : nheb ne5ou 9arth -> hab 5dhe 9ardh\n",
    "def search(prequery, discussions, normm_lemma):\n",
    "    \"\"\"Search and return answers of top 5 similar questions.\"\"\"\n",
    "    # Search\n",
    "    results = []\n",
    "    for disc_id, discussion in enumerate(discussions):\n",
    "        for utter_id, utterance in enumerate(discussion):\n",
    "            doc_tokens = normalize_tokens(re.findall(r\"(\\w+|\\S+)\", utterance[\"question_norm\"].lower()), normm_lemma)\n",
    "            similarity = jaccard_similarity(set(prequery), set(doc_tokens))\n",
    "            results.append((disc_id, utter_id, similarity))\n",
    "    # Sort results by similarity score in descending order and return top 5\n",
    "    results.sort(key=lambda x: x[2], reverse=True)\n",
    "    return results[:5]\n",
    "\n",
    "\n",
    "# def search(query, documents):\n",
    "#     query_tokens = set(normalize_tokens) # fonction de normalisation set query twali set normalisation \n",
    "#     results = []\n",
    "#     for doc_id, doc_content in documents.items():\n",
    "#         doc_tokens = set(doc_content.lower().split())  # fonction de normalisation set query twali set normalisation \n",
    "#         similarity = jaccard_similarity(query_tokens, doc_tokens) \n",
    "#         results.append((doc_id, similarity)) # zid l id ta3 reponse (id: id de la question et sa reponse)\n",
    "\n",
    "# def search(query, documents):\n",
    "#     # query_tokens = set(query.lower().split()) # fonction de normalisation set query twali set normalisation \n",
    "#     results = []\n",
    "#     for doc_id, doc_content in documents.items():\n",
    "#         doc_tokens = set(doc_content.lower().split())  # fonction de normalisation set query twali set normalisation \n",
    "#         similarity = jaccard_similarity(normalized_question_tokens, doc_tokens) \n",
    "#         results.append((doc_id, similarity)) # zid l id ta3 reponse (id: id de la question et sa reponse)\n",
    "        \n",
    "    # results.sort(key=lambda x: x[1], reverse=True)\n",
    "    # return results[:5]  # Return top 5 results (answers of the top 5 selected questions)\n",
    "\n",
    "# Load the messages\n",
    "discussions = read_documents('./res.json')\n",
    "query = \"انا عامل مترسم في شركة خاصةوماذبيا على قرض بدون فائض\" \n",
    "\n",
    "def prevMessages(disc_id, utter_id):\n",
    "    prev_messages = []\n",
    "    for utterance in discussions[disc_id][utter_id-2:utter_id]:\n",
    "        prev_messages.append(utterance[\"question\"])\n",
    "        prev_messages.append(utterance[\"reponse\"])\n",
    "    return prev_messages\n",
    "\n",
    "def topReponses(normquery, discussions, normm_lemma):\n",
    "    # Perform the search and get the top 5 results\n",
    "    search_results = search(normquery, discussions, normm_lemma)\n",
    "    reponses = []\n",
    "    for result in search_results:\n",
    "        disc_id, utter_id, similarity = result\n",
    "        #if similarity>=0.5:\n",
    "        reponses.append(discussions[disc_id][utter_id][\"reponse\"])\n",
    "        #question = discussions[disc_id][utter_id][\"question\"]\n",
    "        \n",
    "        #print(f\"Discussion[{disc_id}, {utter_id}] - Similarity: {similarity}\")\n",
    "        #print(f\"Question: {question}\")\n",
    "        #print(f\"Reponse: {reponse}\")\n",
    "        #print(\"\\n\\n\")\n",
    "    return reponses\n",
    "\n",
    "# Dataset avec le contexte et les dernieres utterances\n",
    "dataset = {\"data\": [{\"paragraphs\": [], \"title\": \"1\"}], \"version\": \"1.1\"}\n",
    "\n",
    "id_qas = 0\n",
    "for disc_id, discussion in enumerate(discussions):\n",
    "    for utter_id, utterance in enumerate(discussion):\n",
    "\n",
    "        # normaliser la requête pour Jaccard\n",
    "        normquery = norm_query_jaccard(utterance[\"question\"], normm_lemma)\n",
    "\n",
    "        # prev utterances\n",
    "        prev_messages = prevMessages(disc_id, utter_id)\n",
    "        # top reponses\n",
    "        top_reponses = topReponses(normquery, discussions, normm_lemma)\n",
    "        if utterance[\"reponse\"] != top_reponses[0]:\n",
    "            #print(utterance[\"question\"])\n",
    "            #print(utterance[\"question_norm\"])\n",
    "            top_reponses.insert(0, utterance[\"reponse\"])\n",
    "\n",
    "        # 6 echantillons: 5 avec la reponse correcte en premier, 1 avec reponses mélangées\n",
    "        for i in range(1+min(len(top_reponses), 5)):\n",
    "\n",
    "            # mélanger les reponses?\n",
    "            answer_start = 0\n",
    "            if i>0:\n",
    "                # Melanger reponses et trouver le nv answer_index\n",
    "                newL = list(enumerate(top_reponses))\n",
    "                random.shuffle(newL)\n",
    "                top_reponses_shuffled = \"\"\n",
    "                trouve_correct = False\n",
    "                for (old_j, rep) in newL:\n",
    "                    if old_j==0:\n",
    "                        answer_start = len(top_reponses_shuffled)\n",
    "                    top_reponses_shuffled += \" \"*bool(top_reponses_shuffled) + top_reponses[old_j] + \" ;\"\n",
    "\n",
    "                    # retrouvé la reponse correcte?\n",
    "                    #if old_j==0:\n",
    "                        #trouve_correct = True\n",
    "                    #elif not trouve_correct:\n",
    "                        #answer_start += len(top_reponses[old_j]) + 3\n",
    "                #top_reponses = top_reponses_shuffled\n",
    "                #else:\n",
    "                #    top_reponses =  \" ; \".join(top_reponses) + \" ;\"\n",
    "                id_qas += 1\n",
    "                context = top_reponses_shuffled #\" ; \".join(top_reponses) + \" ;\"\n",
    "                question = \" ; \".join(prev_messages) + \" ; \" + utterance[\"question\"]\n",
    "                answer_text = utterance[\"reponse\"]\n",
    "                '''\n",
    "                try:\n",
    "                    x = context[answer_start+len(utterance[\"reponse\"])-1]\n",
    "                except:\n",
    "                    #print(answer_start)\n",
    "                    #print(answer_start+len(utterance[\"reponse\"])-1)\n",
    "                    #print(len(context))\n",
    "                    print(\"ERROR\", context[answer_start:answer_start+len(utterance[\"reponse\"])-1])\n",
    "                    pass\n",
    "                '''\n",
    "                # remplir le dataset\n",
    "                dataset[\"data\"][0][\"paragraphs\"].append(\n",
    "                    {\n",
    "                        \"context\": context,\n",
    "                        \"qas\": [\n",
    "                            {\n",
    "                                \"id\": str(id_qas),\n",
    "                                \"question\": question,\n",
    "                                \"answers\": [{\"answer_start\": answer_start, \"text\": answer_text}]\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                )\n",
    "\n",
    "# enregistrer train & test sets\n",
    "nb_train = int(len(dataset[\"data\"][0][\"paragraphs\"])*0.8)\n",
    "\n",
    "random.shuffle(dataset[\"data\"][0][\"paragraphs\"])\n",
    "\n",
    "train_set = {\"data\": [{\"paragraphs\": [], \"title\": \"1\"}], \"version\": \"1.1\"}\n",
    "train_set[\"data\"][0][\"paragraphs\"] = dataset[\"data\"][0][\"paragraphs\"][:nb_train]\n",
    "\n",
    "test_set = {\"data\": [{\"paragraphs\": [], \"title\": \"1\"}], \"version\": \"1.1\"}\n",
    "test_set[\"data\"][0][\"paragraphs\"] = dataset[\"data\"][0][\"paragraphs\"][nb_train:]\n",
    "\n",
    "with open(\"./train.json\", \"w\", encoding=\"utf8\") as file:\n",
    "    json.dump(train_set, file, indent=4)\n",
    "with open(\"./val.json\", \"w\", encoding=\"utf8\") as file:\n",
    "    json.dump(test_set, file, indent=4)\n",
    "with open(\"./test.json\", \"w\", encoding=\"utf8\") as file:\n",
    "    json.dump(test_set, file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
